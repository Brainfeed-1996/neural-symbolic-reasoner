{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Neural-Symbolic Reasoning\n",
    "\n",
    "Advanced neuro-symbolic patterns with constraint satisfaction, knowledge graphs, and explainable AI.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. **Constraint Satisfaction** - Rule validation and consistency\n",
    "2. **Knowledge Graph Integration** - RDF/OWL integration\n",
    "3. **Explanation Generation** - Human-readable reasoning paths\n",
    "4. **Active Learning** - Constraint-guided annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constraint Satisfaction Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Set, Callable\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ConstraintType(Enum):\n",
    "    \"\"\"Types of constraints.\"\"\"\n",
    "    HARD = \"hard\"  # Must be satisfied\n",
    "    SOFT = \"soft\"  # Should be satisfied (penalty if violated)\n",
    "    PREFERENCE = \"preference\"  # Bonus if satisfied\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Constraint:\n",
    "    \"\"\"A constraint on model predictions.\"\"\"\n",
    "    name: str\n",
    "    constraint_type: ConstraintType\n",
    "    description: str\n",
    "    validate_fn: Callable[[Dict], Tuple[bool, str]]\n",
    "    priority: int = 0  # Higher = more important\n",
    "    \n",
    "    def validate(self, predictions: Dict) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate predictions against constraint.\"\"\"\n",
    "        return self.validate_fn(predictions)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConstraintViolation:\n",
    "    \"\"\"A constraint violation.\"\"\"\n",
    "    constraint: str\n",
    "    violated: bool\n",
    "    message: str\n",
    "    severity: str  # error, warning, info\n",
    "\n",
    "\n",
    "class ConstraintSatisfactionEngine:\n",
    "    \"\"\"Engine for constraint satisfaction.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.constraints: List[Constraint] = []\n",
    "        self._constraint_index: Dict[str, Constraint] = {}\n",
    "    \n",
    "    def add_constraint(self, constraint: Constraint):\n",
    "        \"\"\"Add a constraint to the engine.\"\"\"\n",
    "        self.constraints.append(constraint)\n",
        "        self._constraint_index[constraint.name] = constraint\n",
    "        logger.info(f\"Added constraint: {constraint.name}\")\n",
    "    \n",
    "    def validate_all(self, predictions: Dict) -> List[ConstraintViolation]:\n",
    "        \"\"\"Validate predictions against all constraints.\"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        for constraint in sorted(self.constraints, key=lambda c: -c.priority):\n",
    "            valid, message = constraint.validate(predictions)\n",
    "            \n",
    "            if not valid:\n",
    "                severity = \"error\" if constraint.constraint_type == ConstraintType.HARD else \"warning\"\n",
    "                violations.append(ConstraintViolation(\n",
    "                    constraint=constraint.name,\n",
    "                    violated=True,\n",
    "                    message=message,\n",
    "                    severity=severity\n",
    "                ))\n",
    "            else:\n",
    "                violations.append(ConstraintViolation(\n",
    "                    constraint=constraint.name,\n",
    "                    violated=False,\n",
    "                    message=\"Constraint satisfied\",\n",
    "                    severity=\"info\"\n",
    "                ))\n",
    "        \n",
    "        return violations\n",
    "    \n",
    "    def get_hard_violations(self, violations: List[ConstraintViolation]) -> List[ConstraintViolation]:\n",
    "        \"\"\"Filter to only hard constraint violations.\"\"\"\n",
    "        return [v for v in violations if v.severity == \"error\"]\n",
    "    \n",
    "    def is_satisfiable(self, predictions: Dict) -> bool:\n",
    "        \"\"\"Check if predictions satisfy all hard constraints.\"\"\"\n",
    "        violations = self.validate_all(predictions)\n",
    "        return len(self.get_hard_violations(violations)) == 0\n",
    "\n",
    "\n",
    "class ConstraintTemplates:\n",
    "    \"\"\"Pre-defined constraint templates.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mutual_exclusion(\n",
    "        entity_a: str,\n",
    "        entity_b: str,\n",
    "        predicate: str\n",
    "    ) -> Constraint:\n",
    "        \"\"\"Create mutual exclusion constraint.\"\"\"\n",
    "        def validate_fn(predictions: Dict) -> Tuple[bool, str]:\n",
    "            relations = predictions.get(\"relations\", [])\n",
    "            has_a = any(r.get(\"subject\") == entity_a and r.get(\"predicate\") == predicate for r in relations)\n",
    "            has_b = any(r.get(\"subject\") == entity_b and r.get(\"predicate\") == predicate for r in relations)\n",
    "            \n",
    "            if has_a and has_b:\n",
    "                return False, f\"Entities {entity_a} and {entity_b} cannot both have relation {predicate}\"\n",
    "            return True, \"No mutual exclusion violation\"\n",
    "        \n",
    "        return Constraint(\n",
    "            name=f\"mutual_exclusion_{entity_a}_{entity_b}\",\n",
    "            constraint_type=ConstraintType.HARD,\n",
    "            description=f\"{entity_a} and {entity_b} are mutually exclusive for {predicate}\",\n",
    "            validate_fn=validate_fn,\n",
    "            priority=10\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def at_most_one(values: List[str], predicate: str) -> Constraint:\n",
    "        \"\"\"Create at-most-one constraint.\"\"\"\n",
    "        def validate_fn(predictions: Dict) -> Tuple[bool, str]:\n",
    "            relations = predictions.get(\"relations\", [])\n",
    "            count = sum(1 for r in relations if r.get(\"predicate\") == predicate)\n",
    "            \n",
    "            if count > 1:\n",
    "                return False, f\"At most one entity can have {predicate}, found {count}\"\n",
    "            return True, \"At-most-one constraint satisfied\"\n",
    "        \n",
    "        return Constraint(\n",
    "            name=f\"at_most_one_{predicate}\",\n",
    "            constraint_type=ConstraintType.HARD,\n",
    "            description=f\"At most one entity can have {predicate}\",\n",
    "            validate_fn=validate_fn,\n",
    "            priority=10\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def range_constraint(\n",
    "        field: str,\n",
    "        min_val: float,\n",
    "        max_val: float\n",
    "    ) -> Constraint:\n",
    "        \"\"\"Create range constraint for numeric fields.\"\"\"\n",
    "        def validate_fn(predictions: Dict) -> Tuple[bool, str]:\n",
    "            value = predictions.get(field, 0)\n",
    "            \n",
    "            if value < min_val:\n",
    "                return False, f\"{field}={value} is below minimum {min_val}\"\n",
    "            if value > max_val:\n",
    "                False, f\"{field}={value} is above maximum {max_val}\"\n",
    "            return True, f\"{field}={value} is within range [{min_val}, {max_val}]\"\n",
    "        \n",
    "        return Constraint(\n",
    "            name=f\"range_{field}\",\n",
    "            constraint_type=ConstraintType.HARD,\n",
    "            description=f\"{field} must be in [{min_val}, {max_val}]\",\n",
    "            validate_fn=validate_fn\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def type_constraint(\n",
    "        entity: str,\n",
    "        required_types: List[str]\n",
    "    ) -> Constraint:\n",
    "        \"\"\"Create type constraint.\"\"\"\n",
    "        def validate_fn(predictions: Dict) -> Tuple[bool, str]:\n",
    "            entities = predictions.get(\"entities\", [])\n",
    "            target = next((e for e in entities if e.get(\"id\") == entity), None)\n",
    "            \n",
    "            if not target:\n",
    "                return False, f\"Entity {entity} not found\"\n",
    "            \n",
    "            entity_types = target.get(\"types\", [])\n",
    "            if not any(t in required_types for t in entity_types):\n",
    "                return False, f\"Entity {entity} must be one of {required_types}\"\n",
    "            return True, f\"Entity {entity} has valid type\"\n",
    "        \n",
    "        return Constraint(\n",
    "            name=f\"type_{entity}\",\n",
    "            constraint_type=ConstraintType.HARD,\n",
    "            description=f\"Entity {entity} must have type in {required_types}\",\n",
    "            validate_fn=validate_fn\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Knowledge Graph Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class KGEntity:\n",
    "    \"\"\"Knowledge graph entity.\"\"\"\n",
    "    id: str\n",
    "    label: str\n",
    "    types: List[str]\n",
    "    properties: Dict = field(default_factory=dict)\n",
    "    embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "@dataclass\n",
    "class KGRelation:\n",
    "    \"\"\"Knowledge graph relation.\"\"\"\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    confidence: float = 1.0\n",
    "\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    \"\"\"In-memory knowledge graph for reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.entities: Dict[str, KGEntity] = {}\n",
    "        self.relations: List[KGRelation] = []\n",
    "        self.index: Dict[str, List[KGRelation]] = defaultdict(list)\n",
    "    \n",
    "    def add_entity(self, entity: KGEntity):\n",
    "        \"\"\"Add entity to graph.\"\"\"\n",
    "        self.entities[entity.id] = entity\n",
    "        logger.debug(f\"Added entity: {entity.id}\")\n",
    "    \n",
    "    def add_relation(self, relation: KGRelation):\n",
    "        \"\"\"Add relation to graph.\"\"\"\n",
    "        self.relations.append(relation)\n",
    "        self.index[relation.subject].append(relation)\n",
    "        self.index[relation.object].append(relation)  # Also index by object\n",
    "        logger.debug(f\"Added relation: {relation.subject} -> {relation.predicate} -> {relation.object}\")\n",
    "    \n",
    "    def get_entity(self, entity_id: str) -> Optional[KGEntity]:\n",
    "        \"\"\"Get entity by ID.\"\"\"\n",
    "        return self.entities.get(entity_id)\n",
    "    \n",
    "    def get_neighbors(self, entity_id: str, relation: str = None) -> List[Tuple[KGRelation, KGEntity]]:\n",
    "        \"\"\"Get neighboring entities.\"\"\"\n",
    "        neighbors = []\n",
    "        \n",
    "        for rel in self.index.get(entity_id, []):\n",
    "            if relation and rel.predicate != relation:\n",
    "                continue\n",
    "            \n",
    "            target_id = rel.object if rel.subject == entity_id else rel.subject\n",
    "            target = self.get_entity(target_id)\n",
    "            \n",
    "            if target:\n",
    "                neighbors.append((rel, target))\n",
    "        \n",
    "        return neighbors\n",
    "    \n",
    "    def find_path(\n",
    "        self,\n",
    "        source: str,\n",
    "        target: str,\n",
    "        max_depth: int = 3\n",
    "    ) -> List[List[KGRelation]]:\n",
    "        \"\"\"Find paths between two entities (BFS).\"\"\"\n",
    "        from collections import deque\n",
    "        \n",
    "        queue = deque([(source, [source])])\n",
    "        visited = {source}\n",
    "        paths = []\n",
    "        \n",
    "        while queue and len(paths) < 10:\n",
    "            current, path = queue.popleft()\n",
    "            \n",
    "            if current == target:\n",
    "                paths.append(path)\n",
    "                continue\n",
    "            \n",
    "            if len(path) > max_depth:\n",
    "                continue\n",
    "            \n",
    "            for rel in self.index.get(current, []):\n",
    "                next_entity = rel.object if rel.subject == current else rel.subject\n",
    "                \n",
    "                if next_entity not in visited:\n",
    "                    visited.add(next_entity)\n",
    "                    queue.append((next_entity, path + [next_entity]))\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Export graph to dictionary.\"\"\"\n",
    "        return {\n",
    "            \"entities\": {\n",
    "                eid: {\n",
    "                    \"id\": ent.id,\n",
    "                    \"label\": ent.label,\n",
    "                    \"types\": ent.types,\n",
    "                    \"properties\": ent.properties\n",
    "                }\n",
    "                for eid, ent in self.entities.items()\n",
    "            },\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"subject\": r.subject,\n",
    "                    \"predicate\": r.predicate,\n",
    "                    \"object\": r.object,\n",
    "                    \"confidence\": r.confidence\n",
    "                }\n",
    "                for r in self.relations\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class KGReasoner:\n",
    "    \"\"\"Reason over knowledge graph.\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph: KnowledgeGraph):\n",
    "        self.kg = knowledge_graph\n",
    "    \n",
    "    def find_related_entities(\n",
    "        self,\n",
    "        entity_id: str,\n",
    "        relation_types: List[str] = None,\n",
    "        depth: int = 2\n",
    "    ) -> List[Tuple[KGEntity, str, float]]:\n",
    "        \"\"\"Find entities related to a given entity.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        def dfs(current: str, current_depth: int, visited: Set[str], path: List[str]):\n",
    "            if current_depth > depth or current in visited:\n",
    "                return\n",
    "            \n",
    "            visited.add(current)\n",
    "            \n",
    "            for rel, neighbor in self.kg.get_neighbors(current):\n",
    "                if relation_types and rel.predicate not in relation_types:\n",
    "                    continue\n",
    "                \n",
    "                results.append((\n",
    "                    neighbor,\n",
    "                    \" -> \".join(path + [rel.predicate]),\n",
    "                    rel.confidence\n",
    "                ))\n",
    "                \n",
    "                dfs(neighbor.id, current_depth + 1, visited, path + [rel.predicate])\n",
    "        \n",
    "        entity = self.kg.get_entity(entity_id)\n",
    "        if entity:\n",
    "            dfs(entity_id, 0, set(), [entity.label])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def validate_predictions(self, predictions: Dict) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate predictions against knowledge graph.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check entity existence\n",
    "        for entity_id in predictions.get(\"entity_ids\", []):\n",
    "            if not self.kg.get_entity(entity_id):\n",
    "                issues.append(f\"Unknown entity: {entity_id}\")\n",
    "        \n",
    "        # Check relation validity\n",
    "        for rel in predictions.get(\"relations\", []):\n",
    "            subj = self.kg.get_entity(rel.get(\"subject\"))\n",
    "            obj = self.kg.get_entity(rel.get(\"object\"))\n",
    "            \n",
    "            if not subj or not obj:\n",
    "                issues.append(f\"Invalid relation: {rel}\")\n",
    "        \n",
    "        return len(issues) == 0, issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explanation Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExplanationStep:\n",
    "    \"\"\"A single step in an explanation.\"\"\"\n",
    "    step_id: int\n",
    "    description: str\n",
    "    confidence: float\n",
    "    rule_applied: str\n",
    "    evidence: List[str]\n",
    "\n",
    "@dataclass\n",
    "class Explanation:\n",
    "    \"\"\"A complete explanation for a prediction.\"\"\"\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    steps: List[ExplanationStep]\n",
    "    rule_overrides: List[str]\n",
    "    uncertainty_factors: List[str]\n",
    "    \n",
    "    def to_text(self) -> str:\n",
    "        \"\"\"Convert explanation to human-readable text.\"\"\"\n",
    "        lines = []\n",
    "        lines.append(f\"PREDICTION: {self.prediction}\")\n",
    "        lines.append(f\"CONFIDENCE: {self.confidence:.2%}\")\n",
    "        lines.append(\"\\nREASONING CHAIN:\")\n",
    "        \n",
    "        for i, step in enumerate(self.steps, 1):\n",
    "            lines.append(f\"  {i}. [{step.rule_applied}] {step.description}\")\n",
    "            if step.evidence:\n",
    "                for ev in step.evidence[:3]:  # Limit evidence\n",
    "                    lines.append(f\"     → {ev}\")\n",
    "        \n",
    "        if self.rule_overrides:\n",
    "            lines.append(f\"\\nRULE OVERRIDES: {', '.join(self.rule_overrides)}\")\n",
        "        \n",
    "        if self.uncertainty_factors:\n",
    "            lines.append(f\"\\nUNCERTAINTY FACTORS:\")\n",
    "            for uf in self.uncertainty_factors:\n",
    "                lines.append(f\"  • {uf}\")\n",
        "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    \"\"\"Generate human-readable explanations.\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_graph: KnowledgeGraph = None):\n",
    "        self.kg = knowledge_graph\n",
    "        self.rule_registry: Dict[str, str] = {}\n",
    "    \n",
    "    def register_rule(self, rule_id: str, description: str):\n",
    "        \"\"\"Register a rule for explanation.\"\"\"\n",
    "        self.rule_registry[rule_id] = description\n",
    "    \n",
    "    def explain_prediction(\n",
    "        self,\n",
    "        prediction: str,\n",
        "        model_outputs: Dict,\n",
    "        applied_rules: List[str],\n",
    "        constraints_violated: List[str] = None,\n",
    "        uncertainty: float = 0.0\n",
    "    ) -> Explanation:\n",
    "        \"\"\"Generate explanation for a prediction.\"\"\"\n",
    "        \n",
    "        # Build reasoning steps\n",
    "        steps = []\n",
        "        \n",
    "        # Step 1: Neural model contribution\n",
    "        steps.append(ExplanationStep(\n",
    "            step_id=1,\n",
    "            description=\"Neural model analyzed input features\",\n",
    "            confidence=model_outputs.get(\"model_confidence\", 0.8),\n",
    "            rule_applied=\"NEURAL_MODEL\",\n",
    "            evidence=self._extract_evidence(model_outputs)\n",
    "        ))\n",
        "        \n",
        "        # Step 2: Rule-based reasoning\n",
        "        for i, rule in enumerate(applied_rules, 2):\n",
    "            rule_desc = self.rule_registry.get(rule, f\"Rule {rule}\")\n",
    "            steps.append(ExplanationStep(\n",
    "                step_id=i,\n",
    "                description=rule_desc,\n",
    "                confidence=1.0 if rule in applied_rules else 0.0,\n",
    "                rule_applied=rule,\n",
    "                evidence=[]\n",
    "            ))\n",
    "        \n",
    "        # Calculate overall confidence\n",
    "        confidences = [s.confidence for s in steps]\n",
    "        overall_confidence = np.mean(confidences) * (1.0 - uncertainty)\n",
    "        \n",
    "        # Identify uncertainty factors\n",
    "        uncertainty_factors = []\n",
    "        if uncertainty > 0.2:\n",
    "            uncertainty_factors.append(f\"High input uncertainty ({uncertainty:.1%})\")\n",
    "        if constraints_violated:\n",
    "            uncertainty_factors.append(f\"Constraint violations: {len(constraints_violated)}\")\n",
    "        if model_outputs.get(\"low_confidence_features\"):\n",
    "            uncertainty_factors.append(\"Some input features had low model confidence\")\n",
    "        \n",
    "        return Explanation(\n",
    "            prediction=prediction,\n",
    "            confidence=overall_confidence,\n",
    "            steps=steps,\n",
    "            rule_overrides=applied_rules,\n",
    "            uncertainty_factors=uncertainty_factors\n",
    "        )\n",
    "    \n",
    "    def _extract_evidence(self, model_outputs: Dict) -> List[str]:\n",
    "        \"\"\"Extract evidence from model outputs.\"\"\"\n",
    "        evidence = []\n",
        "        \n",
    "        # Top features\n",
    "        top_features = model_outputs.get(\"top_features\", [])[:3]\n",
    "        for feat in top_features:\n",
    "            evidence.append(f\"Feature '{feat.get('name')}' score: {feat.get('score', 0):.3f}\")\n",
        "        \n",
    "        return evidence\n",
    "\n",
    "\n",
    "def demonstrate_neurosymbolic_advanced():\n",
    "    \"\"\"Demo advanced neural-symbolic reasoning.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ADVANCED NEURAL-SYMBOLIC REASONING DEMO\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # 1. Constraint Satisfaction\n",
    "    print(\"\\n[1] Constraint Satisfaction Engine\")\n",
    "    engine = ConstraintSatisfactionEngine()\n",
    "    \n",
    "    # Add mutual exclusion constraint\n",
    "    engine.add_constraint(ConstraintTemplates.mutual_exclusion(\n",
    "        \"Entity_A\", \"Entity_B\", \"located_in\"\n",
    "    ))\n",
    "    \n",
    "    # Add range constraint\n",
    "    engine.add_constraint(ConstraintTemplates.range_constraint(\n",
    "        \"confidence\", 0.0, 1.0\n",
    "    ))\n",
    "    \n",
    "    # Test valid predictions\n",
    "    valid_preds = {\n",
    "        \"relations\": [\n",
    "            {\"subject\": \"Entity_A\", \"predicate\": \"located_in\", \"object\": \"France\"}\n",
    "        ],\n",
    "        \"confidence\": 0.85\n",
    "    }\n",
    "    \n",
    "    violations = engine.validate_all(valid_preds)\n",
    "    print(f\"  Valid predictions violations: {len([v for v in violations if v.violated])}\")\n",
    "    \n",
    "    # Test invalid predictions\n",
    "    invalid_preds = {\n",
    "        \"relations\": [\n",
    "            {\"subject\": \"Entity_A\", \"predicate\": \"located_in\", \"object\": \"France\"},\n",
    "            {\"subject\": \"Entity_B\", \"predicate\": \"located_in\", \"object\": \"Germany\"}\n",
    "        ],\n",
    "        \"confidence\": 1.5  # Out of range\n",
    "    }\n",
    "    \n",
    "    violations = engine.validate_all(invalid_preds)\n",
    "    hard_violations = engine.get_hard_violations(violations)\n",
    "    print(f\"  Invalid predictions hard violations: {len(hard_violations)}\")\n",
    "    \n",
    "    # 2. Knowledge Graph\n",
    "    print(\"\\n[2] Knowledge Graph Reasoning\")\n",
    "    kg = KnowledgeGraph()\n",
    "    \n",
    "    # Add entities\n",
    "    kg.add_entity(KGEntity(\n",
    "        id=\"paris\",\n",
    "        label=\"Paris\",\n",
    "        types=[\"City\", \"Capital\"],\n",
    "        properties={\"population\": 2148000}\n",
    "    ))\n",
    "    kg.add_entity(KGEntity(\n",
    "        id=\"france\",\n",
    "        label=\"France\",\n",
    "        types=[\"Country\"],\n",
    "        properties={\"population\": 67390000}\n",
    "    ))\n",
    "    kg.add_entity(KGEntity(\n",
    "        id=\"europe\",\n",
    "        label=\"Europe\",\n",
    "        types=[\"Continent\"]\n",
    "    ))\n",
    "    \n",
    "    # Add relations\n",
    "    kg.add_relation(KGRelation(\"paris\", \"capital_of\", \"france\"))\n",
    "    kg.add_relation(KGRelation(\"france\", \"located_in\", \"europe\"))\n",
    "    \n",
    "    # Find path\n",
    "    paths = kg.find_path(\"paris\", \"europe\", max_depth=3)\n",
    "    print(f\"  Paris -> Europe paths found: {len(paths)}\")\n",
    "    \n",
    "    # 3. Explanation Generation\n",
    "    print(\"\\n[3] Explanation Generation\")\n",
    "    explainer = ExplanationGenerator(kg)\n",
    "    explainer.register_rule(\"MUTUAL_EXCLUSION\", \"Entities cannot both have this relation\")\n",
    "    \n",
    "    explanation = explainer.explain_prediction(\n",
    "        prediction=\"Paris is the capital of France\",\n",
    "        model_outputs={\"model_confidence\": 0.92, \"top_features\": [\n",
    "            {\"name\": \"location_context\", \"score\": 0.88}\n",
    "        ]},\n",
    "        applied_rules=[\"CAPITAL_VERIFICATION\"],\n",
    "        constraints_violated=[],\n",
    "        uncertainty=0.08\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + explanation.to_text())\n",
    "    \n",
    "    return {\n",
    "        \"constraint_engine\": engine,\n",
    "        \"knowledge_graph\": kg,\n",
    "        \"explainer\": explainer\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_neurosymbolic_advanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Constraint Satisfaction**: Hard/soft constraints with validation\n",
    "- **Knowledge Graphs**: In-memory graph with path finding\n",
    "- **Explanation Generation**: Human-readable reasoning chains\n",
    "- **Integration**: Combined neuro-symbolic reasoning pipeline\n",
    "\n",
    "The module provides enterprise-grade explainable AI capabilities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
